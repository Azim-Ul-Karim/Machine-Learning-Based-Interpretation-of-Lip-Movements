{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b34eoTj2v3Zj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Required Packages"
      ],
      "metadata": {
        "id": "FS4APg6KxdbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8RyV9LUxbug"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python matplotlib imageio gdown tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Libraries"
      ],
      "metadata": {
        "id": "P1ngyjodxq-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from matplotlib import pyplot as plt\n",
        "import imageio"
      ],
      "metadata": {
        "id": "4Y68Zx_ix3FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Unzip Dataset"
      ],
      "metadata": {
        "id": "PdBVdgd_ymXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL -O GRID_dataset.zip\n",
        "!unzip -q GRID_dataset.zip"
      ],
      "metadata": {
        "id": "AGfCEqqjypbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Vocabulary and Character Mapping Layer"
      ],
      "metadata": {
        "id": "TaOw2Mn6yu9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of all valid characters in the dataset\n",
        "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
        "\n",
        "# Map characters to numbers and vice versa\n",
        "char_to_num = tf.keras.layers.StringLookup(\n",
        "    vocabulary=vocab,\n",
        "    oov_token=\"\"\n",
        ")\n",
        "num_to_char = tf.keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(),\n",
        "    oov_token=\"\",\n",
        "    invert=True\n",
        ")\n",
        "\n",
        "# Show vocabulary info\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()}\\n\"\n",
        "    f\"(Size = {char_to_num.vocabulary_size()})\"\n",
        ")"
      ],
      "metadata": {
        "id": "gPKkYhnNy_fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Video Loading Function"
      ],
      "metadata": {
        "id": "qtXkEc5X1xUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(path: str) -> tf.Tensor:\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "\n",
        "    # Loop through all video frames\n",
        "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or frame is None:\n",
        "            print(f\"Skipping invalid frame in {path}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "\n",
        "            # Ensure frame is valid RGB\n",
        "            if frame.ndim != 3 or frame.shape[2] != 3:\n",
        "                raise ValueError(f\"Expected RGB frame but got shape {frame.shape}\")\n",
        "            frame = tf.convert_to_tensor(frame, dtype=tf.uint8)\n",
        "\n",
        "            # Check for minimum dimensions before cropping\n",
        "            if frame.shape[0] < 236 or frame.shape[1] < 220:\n",
        "                raise ValueError(f\"Invalid frame shape: {frame.shape}\")\n",
        "\n",
        "            # Convert to grayscale and crop region of interest (RoI) (46x140)\n",
        "            frame = tf.image.rgb_to_grayscale(frame)\n",
        "            frame = frame[190:236, 80:220, :]\n",
        "\n",
        "            # Check final frame shape\n",
        "            if frame.shape != (46, 140, 1):\n",
        "                raise ValueError(f\"Frame shape mismatch after crop: {frame.shape}\")\n",
        "\n",
        "            frames.append(frame)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Frame error in {path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Return zero tensor if no valid frames found\n",
        "    if not frames:\n",
        "        print(f\"Warning: No frames in {path}\")\n",
        "        return tf.zeros((75, 46, 140, 1), dtype=tf.float32)\n",
        "\n",
        "    # Pad or trim to exactly 75 frames\n",
        "    if len(frames) < 75:\n",
        "        frames += [frames[-1]] * (75 - len(frames))\n",
        "    else:\n",
        "        frames = frames[:75]\n",
        "\n",
        "    # Normalize video frames\n",
        "    frames = tf.stack(frames)\n",
        "    frames = tf.cast(frames, tf.float32)\n",
        "    mean = tf.reduce_mean(frames)\n",
        "    std = tf.math.reduce_std(frames)\n",
        "    return (frames - mean) / (std + 1e-6)"
      ],
      "metadata": {
        "id": "DuIOVFOTzyMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Alignment Loading Function"
      ],
      "metadata": {
        "id": "haQdkbJq0B2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_alignments(path: str) -> tf.Tensor:\n",
        "\n",
        "    # Read alignment file\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Extract non-silence tokens\n",
        "    tokens = \"\"\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) >= 3 and parts[2] != \"sil\":\n",
        "            tokens += \" \" + parts[2]\n",
        "    tokens = tokens.strip()\n",
        "\n",
        "    # Return empty tensor if no tokens\n",
        "    if not tokens:\n",
        "        return tf.constant([], dtype=tf.int64)\n",
        "\n",
        "    # Convert characters to numeric sequence\n",
        "    chars = tf.reshape(tf.strings.unicode_split(tokens, input_encoding=\"UTF-8\"), (-1,))\n",
        "    return char_to_num(chars)"
      ],
      "metadata": {
        "id": "42Enln2Y0Fqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap Data Loading for TensorFlow"
      ],
      "metadata": {
        "id": "SEBbsBJZ2ZCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path: tf.Tensor):\n",
        "\n",
        "    # Decode file path from tensor to string\n",
        "    path = path.numpy().decode('utf-8')\n",
        "    file_name = os.path.splitext(os.path.basename(path))[0]\n",
        "\n",
        "    # Construct video and alignment paths\n",
        "    video_path = os.path.join('data', 's1', f'{file_name}.mpg')\n",
        "    alignment_path = os.path.join('data', 'alignments', 's1', f'{file_name}.align')\n",
        "\n",
        "    # Load video frames and label sequence\n",
        "    frames = load_video(video_path)\n",
        "    alignments = load_alignments(alignment_path)\n",
        "    return frames, alignments\n",
        "\n",
        "# TensorFlow wrapper to call Python function inside data pipeline\n",
        "def mappable_function(path):\n",
        "    return tf.py_function(load_data, [path], (tf.float32, tf.int64))"
      ],
      "metadata": {
        "id": "yy7SNsb62a_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Data Pipeline"
      ],
      "metadata": {
        "id": "r0lU4bPf5c5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset of video files and shuffle\n",
        "data = tf.data.Dataset.list_files('./data/s1/*.mpg')\n",
        "data = data.shuffle(500, reshuffle_each_iteration=False)\n",
        "\n",
        "# Map video and label loader function\n",
        "data = data.map(mappable_function)\n",
        "\n",
        "# Batch and pad inputs to 75×46×140×1 shape, and prefetch for performance\n",
        "data = data.padded_batch(batch_size=2, padded_shapes=([75, 46, 140, 1], [40]))\n",
        "data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Split into training and test sets\n",
        "train = data.take(450)\n",
        "test = data.skip(450)"
      ],
      "metadata": {
        "id": "6ZQNXqYQ5eEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Sample Video and Alignment"
      ],
      "metadata": {
        "id": "b34eoTj2v3Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load one batch\n",
        "frames, alignments = data.as_numpy_iterator().next()\n",
        "sample = data.as_numpy_iterator()\n",
        "val = sample.next()\n",
        "\n",
        "# Normalize video to 0-255 and save as GIF\n",
        "video = val[0][0]\n",
        "video = np.squeeze(video, axis=-1)\n",
        "vmin, vmax = video.min(), video.max()\n",
        "video_norm = (video - vmin) / (vmax - vmin)\n",
        "video_uint8 = (video_norm * 255).astype(np.uint8)\n",
        "\n",
        "# Save video as animated GIF\n",
        "imageio.mimsave('./Animation.gif', list(video_uint8), fps=10)\n",
        "\n",
        "# Show one specific frame\n",
        "plt.imshow(video_uint8[30])\n",
        "\n",
        "# Decode and print alignment for first example\n",
        "decoded = tf.strings.reduce_join([num_to_char(word) for word in val[1][0]])\n",
        "print(decoded.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "HiFQPWDmv3yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Deep Neural Network (3D-CNN + BiLSTM Model)"
      ],
      "metadata": {
        "id": "_Chxl-f76qCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import essential model-building layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, Activation, MaxPool3D, TimeDistributed, Flatten, Bidirectional, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Sequential model architecture combining 3D CNN, TimeDistributed Flatten, and BiLSTM layers\n",
        "model = Sequential([\n",
        "\n",
        "    # First 3D convolution layer with 128 filters\n",
        "    Conv3D(128, 3, padding='same', input_shape=(75, 46, 140, 1)),\n",
        "    Activation('relu'),\n",
        "    MaxPool3D((1, 2, 2)),\n",
        "\n",
        "    # Second 3D convolution layer with 256 filters\n",
        "    Conv3D(256, 3, padding='same'),\n",
        "    Activation('relu'),\n",
        "    MaxPool3D((1, 2, 2)),\n",
        "\n",
        "    # Third 3D convolution layer with 75 filters\n",
        "    Conv3D(75, 3, padding='same'),\n",
        "    Activation('relu'),\n",
        "    MaxPool3D((1, 2, 2)),\n",
        "\n",
        "    # Flatten the spatial dimensions\n",
        "    TimeDistributed(Flatten()),\n",
        "\n",
        "    # First bidirectional LSTM to learn temporal dependencies\n",
        "    Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='Orthogonal')),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Second bidirectional LSTM\n",
        "    Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='Orthogonal')),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output layer with softmax over character vocabulary\n",
        "    Dense(char_to_num.vocabulary_size() + 1, activation='softmax', kernel_initializer='he_normal')\n",
        "])"
      ],
      "metadata": {
        "id": "yNR1G4cK6Vp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model architecture summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SRiHqWZk7GZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "WSBS6wsH7G8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Learning rate scheduler to reduce LR\n",
        "def scheduler(epoch: int):\n",
        "    if epoch < 30:\n",
        "        return 1e-3\n",
        "    else:\n",
        "        return 1e-4\n",
        "\n",
        "# Wrap scheduler in a TensorFlow callback\n",
        "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "metadata": {
        "id": "RDJtMkwP7KCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define (Connectionist Temporal Classification) CTC Loss Function"
      ],
      "metadata": {
        "id": "SJUlYan87OyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "\n",
        "    # Compute batch size and sequence lengths for predictions and labels\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=tf.int64)\n",
        "    input_len = tf.cast(tf.shape(y_pred)[1], dtype=tf.int64)\n",
        "    label_len = tf.cast(tf.shape(y_true)[1], dtype=tf.int64)\n",
        "\n",
        "    # Input and label lengths are constant per batch\n",
        "    input_length = input_len * tf.ones(shape=(batch_len, 1), dtype=tf.int64)\n",
        "    label_length = label_len * tf.ones(shape=(batch_len, 1), dtype=tf.int64)\n",
        "\n",
        "    # Return mean CTC loss over the batch\n",
        "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "b7zKkTvJ7SQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Callback to Decode and Display Predictions\n"
      ],
      "metadata": {
        "id": "AYJQKvIu7UMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProduceExample(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # Initialize with dataset (store dataset directly, not iterator)\n",
        "    def __init__(self, dataset) -> None:\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
        "        try:\n",
        "\n",
        "            # Create a fresh iterator and get one batch of data\n",
        "            data = next(iter(self.dataset))\n",
        "            # Unpack batch into videos and labels\n",
        "            videos, labels = data\n",
        "\n",
        "            # Predict on videos using the current model\n",
        "            yhat = self.model.predict(videos)\n",
        "\n",
        "            # Decode predicted sequences using CTC greedy decoding\n",
        "            decoded = tf.keras.backend.ctc_decode(\n",
        "                yhat,\n",
        "                input_length=[yhat.shape[1]] * yhat.shape[0],\n",
        "                greedy=True\n",
        "            )[0][0].numpy()\n",
        "\n",
        "            # Iterate over first two examples or fewer\n",
        "            for i in range(min(2, len(yhat))):\n",
        "                # Convert original labels indices to readable text\n",
        "                original = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n",
        "                # Convert decoded predictions indices to readable text\n",
        "                predicted = tf.strings.reduce_join(num_to_char(decoded[i])).numpy().decode(\"utf-8\")\n",
        "\n",
        "                # Print the original and predicted texts\n",
        "                print(\"Original:\", original)\n",
        "                print(\"Prediction:\", predicted)\n",
        "                # Print a separator for readability\n",
        "                print(\"~\" * 100)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle exceptions gracefully without stopping training\n",
        "            print(f\"[Callback Error] Skipping example callback due to: {e}\")\n",
        "\n",
        "# Show predictions after each epoch using test set\n",
        "example_callback = ProduceExample(test)"
      ],
      "metadata": {
        "id": "Z2FUmMH67Y_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Character-Level Accuracy"
      ],
      "metadata": {
        "id": "dCTkAxvj7bIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class CharacterAccuracyCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store dataset to compute accuracy on samples\n",
        "        self.dataset = dataset\n",
        "        self.accuracy_per_epoch = []\n",
        "\n",
        "    def decode_seq(self, seq):\n",
        "\n",
        "        # Convert sequence indices to characters, ignore padding (0)\n",
        "        return [num_to_char(c).numpy().decode('utf-8') for c in seq if c != 0]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_true_chars = []\n",
        "        y_pred_chars = []\n",
        "\n",
        "        # Take only 5 batches for faster evaluation\n",
        "        for batch in self.dataset.take(5):\n",
        "            videos, labels = batch\n",
        "            yhat = self.model.predict(videos)\n",
        "\n",
        "            # Decode predictions using greedy CTC decoding\n",
        "            decoded = tf.keras.backend.ctc_decode(\n",
        "                yhat,\n",
        "                input_length=[yhat.shape[1]] * yhat.shape[0],\n",
        "                greedy=True\n",
        "            )[0][0].numpy()\n",
        "\n",
        "            # Compare each true and predicted sequence character-wise\n",
        "            for true_seq, pred_seq in zip(labels.numpy(), decoded):\n",
        "                true_dec = self.decode_seq(true_seq)\n",
        "                pred_dec = self.decode_seq(pred_seq)\n",
        "                min_len = min(len(true_dec), len(pred_dec))\n",
        "                y_true_chars.extend(true_dec[:min_len])\n",
        "                y_pred_chars.extend(pred_dec[:min_len])\n",
        "\n",
        "        # Compute accuracy score and save it\n",
        "        acc = accuracy_score(y_true_chars, y_pred_chars)\n",
        "        self.accuracy_per_epoch.append(acc)\n",
        "        print(f\"\\n[Epoch {epoch+1}] Character-Level Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Initialize callback with test dataset\n",
        "acc_callback = CharacterAccuracyCallback(test)"
      ],
      "metadata": {
        "id": "M8dujbhW7chy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model Checkpoint and Compile Model"
      ],
      "metadata": {
        "id": "qVd54_d68qC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to save checkpoints\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create checkpoint directory inside Drive\n",
        "checkpoint_dir = '/content/drive/MyDrive/Labiomancy_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Setup checkpoint callback to save best model by validation loss\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'Best_model.keras'),\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Compile model with Adam optimizer and CTC loss\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)"
      ],
      "metadata": {
        "id": "nG3IcjkG8srb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Training History"
      ],
      "metadata": {
        "id": "WszSC0gT7exM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "class HistorySaverCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, save_path):\n",
        "        super().__init__()\n",
        "        self.save_path = save_path\n",
        "        self.history = {}\n",
        "\n",
        "        # Load existing history if available\n",
        "        if os.path.exists(self.save_path):\n",
        "            with open(self.save_path, 'rb') as f:\n",
        "                self.history = pickle.load(f)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        # Append current epoch metrics to history dictionary\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        # Save updated history to disk\n",
        "        with open(self.save_path, 'wb') as f:\n",
        "            pickle.dump(self.history, f)\n",
        "\n",
        "        # Confirm that history was saved successfully\n",
        "        print(f\"[Epoch {epoch+1}] History saved to {self.save_path}\")\n",
        "\n",
        "# Create and initialize the callback with path to Drive\n",
        "history_saver = HistorySaverCallback('/content/drive/MyDrive/Labiomancy_checkpoints/History_live.pkl')"
      ],
      "metadata": {
        "id": "ppzoRj1K7gDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "hPAaI0wO7lvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training with all callbacks\n",
        "history = model.fit(\n",
        "    train,\n",
        "    validation_data=test,\n",
        "    epochs=70,\n",
        "    callbacks=[acc_callback, checkpoint_callback, schedule_callback, example_callback, history_saver]\n",
        ")"
      ],
      "metadata": {
        "id": "I52wLRLJ7nmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}